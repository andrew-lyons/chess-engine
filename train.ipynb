{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import os\n",
    "\n",
    "TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moves.txt', 'r') as infile:\n",
    "    moves = infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(moves))\n",
    "\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None\n",
    ")\n",
    "\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(moves, 'UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([24  8  6 ...  2  2  1], shape=(3283587,), dtype=int64)\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n"
     ]
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "print(all_ids)\n",
    "\n",
    "# for ids in ids_dataset.take(222):\n",
    "#     print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
    "\n",
    "print(ids_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 397\n",
    "examples_per_epoch = len(moves)\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function split_input_target at 0x1572ff310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function split_input_target at 0x1572ff310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "# for i in dataset.as_numpy_iterator():\n",
    "#     for arr in i:\n",
    "#         print(text_from_ids(arr).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-23 13:22:29.616101: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-23 13:22:29.616245: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 396, 34) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  8704      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  34850     \n",
      "=================================================================\n",
      "Total params: 3,981,858\n",
      "Trainable params: 3,981,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (100, 396, 34)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         3.5263016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33.997997"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n",
    "\n",
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x157274b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x157274b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "82/82 [==============================] - 2119s 26s/step - loss: 2.9003 - accuracy: 0.3454\n",
      "Epoch 2/120\n",
      "82/82 [==============================] - 2174s 26s/step - loss: 1.1193 - accuracy: 0.5966\n",
      "Epoch 3/120\n",
      "82/82 [==============================] - 2161s 26s/step - loss: 0.8070 - accuracy: 0.7215\n",
      "Epoch 4/120\n",
      "82/82 [==============================] - 2296s 28s/step - loss: 0.6967 - accuracy: 0.7517\n",
      "Epoch 5/120\n",
      "82/82 [==============================] - 2179s 27s/step - loss: 0.6676 - accuracy: 0.7620\n",
      "Epoch 6/120\n",
      "82/82 [==============================] - 2281s 28s/step - loss: 0.6442 - accuracy: 0.7697\n",
      "Epoch 7/120\n",
      "82/82 [==============================] - 2219s 27s/step - loss: 0.6236 - accuracy: 0.7772\n",
      "Epoch 8/120\n",
      "82/82 [==============================] - 2242s 27s/step - loss: 0.6054 - accuracy: 0.7843\n",
      "Epoch 9/120\n",
      "82/82 [==============================] - 2379s 29s/step - loss: 0.5896 - accuracy: 0.7902\n",
      "Epoch 10/120\n",
      "82/82 [==============================] - 2273s 28s/step - loss: 0.5771 - accuracy: 0.7946\n",
      "Epoch 11/120\n",
      "82/82 [==============================] - 2150s 26s/step - loss: 0.5671 - accuracy: 0.7979\n",
      "Epoch 12/120\n",
      "82/82 [==============================] - 2294s 28s/step - loss: 0.5578 - accuracy: 0.8009\n",
      "Epoch 13/120\n",
      "82/82 [==============================] - 2235s 27s/step - loss: 0.5491 - accuracy: 0.8038\n",
      "Epoch 14/120\n",
      "82/82 [==============================] - 2272s 28s/step - loss: 0.5416 - accuracy: 0.8064\n",
      "Epoch 15/120\n",
      "82/82 [==============================] - 2355s 29s/step - loss: 0.5347 - accuracy: 0.8091\n",
      "Epoch 16/120\n",
      "82/82 [==============================] - 2366s 29s/step - loss: 0.5289 - accuracy: 0.8113\n",
      "Epoch 17/120\n",
      "82/82 [==============================] - 2292s 28s/step - loss: 0.5222 - accuracy: 0.8137\n",
      "Epoch 18/120\n",
      "82/82 [==============================] - 2281s 28s/step - loss: 0.5154 - accuracy: 0.8164\n",
      "Epoch 19/120\n",
      "82/82 [==============================] - 2338s 29s/step - loss: 0.5100 - accuracy: 0.8185\n",
      "Epoch 20/120\n",
      "82/82 [==============================] - 2398s 29s/step - loss: 0.5045 - accuracy: 0.8205\n",
      "Epoch 21/120\n",
      "82/82 [==============================] - 2312s 28s/step - loss: 0.4990 - accuracy: 0.8225\n",
      "Epoch 22/120\n",
      "82/82 [==============================] - 2317s 28s/step - loss: 0.4935 - accuracy: 0.8246\n",
      "Epoch 23/120\n",
      "82/82 [==============================] - 2400s 29s/step - loss: 0.4885 - accuracy: 0.8264\n",
      "Epoch 24/120\n",
      "82/82 [==============================] - 2345s 29s/step - loss: 0.4845 - accuracy: 0.8278\n",
      "Epoch 25/120\n",
      "82/82 [==============================] - 2316s 28s/step - loss: 0.4823 - accuracy: 0.8287\n",
      "Epoch 26/120\n",
      "82/82 [==============================] - 2306s 28s/step - loss: 0.4782 - accuracy: 0.8303\n",
      "Epoch 27/120\n",
      "82/82 [==============================] - 2344s 29s/step - loss: 0.4730 - accuracy: 0.8318\n",
      "Epoch 28/120\n",
      "82/82 [==============================] - 2292s 28s/step - loss: 0.4675 - accuracy: 0.8341\n",
      "Epoch 29/120\n",
      "82/82 [==============================] - 2259s 28s/step - loss: 0.4628 - accuracy: 0.8357\n",
      "Epoch 30/120\n",
      "82/82 [==============================] - 2245s 27s/step - loss: 0.4589 - accuracy: 0.8371\n",
      "Epoch 31/120\n",
      "82/82 [==============================] - 2216s 27s/step - loss: 0.4566 - accuracy: 0.8381\n",
      "Epoch 32/120\n",
      "82/82 [==============================] - 2265s 28s/step - loss: 0.4531 - accuracy: 0.8392\n",
      "Epoch 33/120\n",
      "82/82 [==============================] - 2341s 29s/step - loss: 0.4483 - accuracy: 0.8410\n",
      "Epoch 34/120\n",
      "82/82 [==============================] - 2320s 28s/step - loss: 0.4441 - accuracy: 0.8425\n",
      "Epoch 35/120\n",
      "82/82 [==============================] - 2253s 27s/step - loss: 0.4412 - accuracy: 0.8435\n",
      "Epoch 36/120\n",
      "82/82 [==============================] - 2273s 28s/step - loss: 0.4374 - accuracy: 0.8448\n",
      "Epoch 37/120\n",
      "82/82 [==============================] - 2355s 29s/step - loss: 0.4338 - accuracy: 0.8462\n",
      "Epoch 38/120\n",
      "82/82 [==============================] - 2370s 29s/step - loss: 0.4316 - accuracy: 0.8470\n",
      "Epoch 39/120\n",
      "82/82 [==============================] - 2248s 27s/step - loss: 0.4272 - accuracy: 0.8487\n",
      "Epoch 40/120\n",
      "82/82 [==============================] - 2256s 27s/step - loss: 0.4221 - accuracy: 0.8504\n",
      "Epoch 41/120\n",
      "60/82 [====================>.........] - ETA: 9:45 - loss: 0.4177 - accuracy: 0.8516 "
     ]
    }
   ],
   "source": [
    "EPOCHS = 120\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49be06992e31b59305782b491bf9385f756e36c49d6ab51d1a994d7ce0ae2be4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tfmac': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
